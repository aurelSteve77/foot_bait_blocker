{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some scripts to get data from facebook in particulars comments\n",
    "\n",
    "* inspired by https://python.gotrained.com/scraping-facebook-posts-comments/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Librairies used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import re\n",
    "import json\n",
    "import logging\n",
    "import bs4 #used for web scrapping\n",
    "import collections\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define some constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URL = 'https://mobile.facebook.com'\n",
    "LOGIN_FORM_URL = '/login/device-based/login/async/?refsrc=deprecated&lwv=100'\n",
    "CREDENTIALS  = {\"email\" : \"avomoa@gmail.com\", \"pass\" : \"ChristIsLord77\"} # to change when publishing on github\n",
    "PROFILE_NAME  = \"kylianmbappeofficiel\"\n",
    "\n",
    "PROFILES_URL = [f'{BASE_URL}/{PROFILE_NAME}/']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define some useful functions and items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FacebookScrapper:\n",
    "    \n",
    "    def __init__(self, base_url):\n",
    "        self.isAuth = False\n",
    "        self.session = requests.session()\n",
    "        self.maxAuthAttempts = 10\n",
    "        self.sleepTime = 3\n",
    "        self.base_url = base_url\n",
    "        self.__post_ids__ = 'recent'\n",
    "    \n",
    "    def login(self, credentials, login_form_url):\n",
    "        \"\"\"\n",
    "        Function to login in orde to been able to scrap data\n",
    "        \"\"\"\n",
    "        #build some variables\n",
    "        params = {\"email\" : credentials['email'], \"pass\" : credentials['pass']}\n",
    "        \n",
    "        #try to log still max attempts\n",
    "        for i in range(self.maxAuthAttempts):\n",
    "            #sleep to avoid to request to fast\n",
    "            time.sleep(self.sleepTime)\n",
    "            \n",
    "            #make the request\n",
    "            logged_request = self.session.post(self.base_url + login_form_url, data = params)\n",
    "            \n",
    "            if logged_request.ok:\n",
    "                print('login successfully')\n",
    "                self.isAuth = True\n",
    "                break\n",
    "            else:\n",
    "                print('something went wrong, try another ....')\n",
    "            \n",
    "    \n",
    "    def get_bs(self, url):\n",
    "        \"\"\"\n",
    "        Make a GET request and use the session to get some pages by beautiful soup\n",
    "        \"\"\"\n",
    "        r = None\n",
    "        for i in range(self.maxAuthAttempts):\n",
    "            r = self.session.get(url) #get with requests\n",
    "            time.sleep(self.sleepTime) #sleep to avoir going to fast\n",
    "            if r.ok:\n",
    "                break;\n",
    "        return BeautifulSoup(r.text, 'lxml')\n",
    "    \n",
    "    def scrap_post(self, post_url):\n",
    "        post_data = OrderedDict()\n",
    "        \n",
    "        post_bs = self.get_bs(self.base_url + post_url)\n",
    "        \n",
    "        time.sleep(self.sleepTime)\n",
    "        \n",
    "        # Here we populate the OrderedDict object\n",
    "        post_data['url'] = post_url\n",
    "        \n",
    "        try:\n",
    "            post_text_element = post_bs.find('div', id='u_0_0').div\n",
    "            string_groups = [p.strings for p in post_text_element.find_all('p')]\n",
    "            strings = [repr(string) for group in string_groups for string in group]\n",
    "            post_data['text'] = strings\n",
    "        except Exception:\n",
    "            post_data['text'] = []\n",
    "\n",
    "        try:\n",
    "            post_data['media_url'] = post_bs.find('div', id='u_0_0').find('a')['href']\n",
    "        except Exception:\n",
    "            post_data['media_url'] = ''\n",
    "\n",
    "\n",
    "        try:\n",
    "            post_data['comments'] = extract_comments(session, base_url, post_bs, post_url)\n",
    "        except Exception:\n",
    "            post_data['comments'] = []\n",
    "\n",
    "        return dict(post_data)\n",
    "        \n",
    "    \n",
    "    def posts_completed(self, scraped_posts, limit):\n",
    "        \"\"\"Returns true if the amount of posts scraped from\n",
    "        profile has reached its limit.\n",
    "        \"\"\"\n",
    "        if len(scraped_posts) == limit:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    def extract_comments(self, post_bs, post_url):\n",
    "        \"\"\"Extracts all coments from post\n",
    "        \"\"\"\n",
    "        comments = list()\n",
    "        show_more_url = post_bs.find('a', href=re.compile('/story\\.php\\?story'))['href']\n",
    "        first_comment_page = True\n",
    "\n",
    "        logging.info('Scraping comments from {}'.format(post_url))\n",
    "        while True:\n",
    "            logging.info('[!] Scraping comments.')\n",
    "            time.sleep(self.sleepTime + 2)\n",
    "            if first_comment_page:\n",
    "                first_comment_page = False\n",
    "            else:\n",
    "                post_bs = self.get_bs(self.base_url + show_more_url)\n",
    "                time.sleep(self.sleepTime)\n",
    "\n",
    "            try:\n",
    "                comments_elements = post_bs.find('div', id=re.compile('composer')).next_sibling\\\n",
    "                    .find_all('div', id=re.compile('^\\d+'))\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "            if len(comments_elements) != 0:\n",
    "                logging.info('[!] There are comments.')\n",
    "            else:\n",
    "                break\n",
    "\n",
    "            for comment in comments_elements:\n",
    "                comment_data = OrderedDict()\n",
    "                comment_data['text'] = list()\n",
    "                try:\n",
    "                    comment_strings = comment.find('h3').next_sibling.strings\n",
    "                    for string in comment_strings:\n",
    "                        comment_data['text'].append(string)\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "                try:\n",
    "                    media = comment.find('h3').next_sibling.next_sibling.children\n",
    "                    if media is not None:\n",
    "                        for element in media:\n",
    "                            comment_data['media_url'] = element['src']\n",
    "                    else:\n",
    "                        comment_data['media_url'] = ''\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "                comment_data['profile_name'] = comment.find('h3').a.string\n",
    "                comment_data['profile_url'] = comment.find('h3').a['href'].split('?')[0]\n",
    "                comments.append(dict(comment_data))\n",
    "\n",
    "            show_more_url = post_bs.find('a', href=re.compile('/story\\.php\\?story'))\n",
    "            if 'View more' in show_more_url.text:\n",
    "                logging.info('[!] More comments.')\n",
    "                show_more_url = show_more_url['href']\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        return comments\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    def crawl_profile(self, profile_name, post_limit = 10):\n",
    "        \"\"\"\n",
    "        Function to crawl a profile or a page and retrieve it's posts\n",
    "        \"\"\"\n",
    "        #get the page corresponding to the profile\n",
    "        profile_bs = self.get_bs(self.base_url + '/' + profile_name + '/')\n",
    "        \n",
    "        #some variables\n",
    "        n_scraped_posts = 0\n",
    "        scraped_posts = []\n",
    "        posts_id = None\n",
    "        \n",
    "        #loop to the post_limit\n",
    "        for i in range(post_limit):\n",
    "            print(f\"post {profile_bs}\")\n",
    "            try:\n",
    "                posts_id = 'recent'\n",
    "                posts = profile_bs.find('div', id=posts_id).div.div.contents\n",
    "            except:\n",
    "                posts_id = 'structured_composer_async_container'\n",
    "                posts = profile_bs.find('div', id=posts_id).div.div.contents\n",
    "            \n",
    "            #get the posts url\n",
    "            posts_urls = [a['href'] for a in profile_bs.find_all('a', text='Full Story')]\n",
    "            \n",
    "            for posts_url in posts_urls:\n",
    "                try:\n",
    "                    post_data = scrape_post(post_url)\n",
    "                    scraped_posts.append(post_data)\n",
    "                except Exception as e:\n",
    "                    #logging.info('Error: {}'.format(e))\n",
    "                    pass\n",
    "                n_scraped_posts += 1\n",
    "                if posts_completed(scraped_posts, post_limit):\n",
    "                    break\n",
    "            show_more_posts_url = None\n",
    "            if not posts_completed(scraped_posts, post_limit):\n",
    "                show_more_posts_url = profile_bs.find('div', id=posts_id).next_sibling.a['href']\n",
    "                profile_bs = self.get_bs(self.base_url + show_more_posts_url)\n",
    "                time.sleep(self.sleepTime)\n",
    "            else:\n",
    "                break\n",
    "        return scraped_posts\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "fscrapper = FacebookScrapper(base_url=BASE_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "login successfully\n"
     ]
    }
   ],
   "source": [
    "fscrapper.login(credentials=CREDENTIALS, login_form_url=LOGIN_FORM_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "post <?xml version=\"1.0\" encoding=\"utf-8\"?><!DOCTYPE html PUBLIC \"-//WAPFORUM//DTD XHTML Mobile 1.0//EN\" \"http://www.wapforum.org/DTD/xhtml-mobile10.dtd\">\n",
      "<html lang=\"fr\" xmlns=\"http://www.w3.org/1999/xhtml\"><head><title>Page introuvable</title><meta content=\"default\" id=\"meta_referrer\" name=\"referrer\"/><style nonce=\"SlEEnX9q\" type=\"text/css\">/*<![CDATA[*/.r{background-color:#fff;border-bottom:1px solid;border-color:#ebedf0;padding:5px 8px 23px 8px;text-align:center;}.s{padding:20px;}.t{text-align:center;}.v{appearance:none;background:none;display:inline-block;font-size:12px;height:28px;line-height:28px;margin:0;overflow:visible;padding:0 9px;text-align:center;vertical-align:top;white-space:nowrap;}.b .v{border-radius:2px;}.z,.bk,a.z,a.bk,html .b a.z,html .b a.bk{color:#fff;}.b .bk{background-color:#4080ff;border:1px solid #4476e8;}.b a.bk:hover,.b .bk:hover{background-color:#4580ef;}.b .z{background-color:#4267b2;border:1px solid #365899;}.b a.z:hover,.b .z:hover{background-color:#465e91;}.z[disabled]{color:#899bc1;}.bk[disabled]{color:#91b4fd;}.b .z[disabled]:hover{background-color:#4267b2;}.b .bk[disabled]:hover{background-color:#4080ff;}.v .bb{display:inline-block;}.b .v .bb{display:inline-block;margin-top:0;vertical-align:middle;}.b a.v::after{content:\"\";display:inline-block;height:100%;vertical-align:middle;}.v .bb{line-height:20px;margin-top:4px;}.b .v{padding:0 8px;}.b a.v{height:26px;line-height:26px;}.bb{pointer-events:none;}.b a,.b a:visited{color:#3b5998;text-decoration:none;}.b .inv,.b .inv:visited{color:#fff;}.b a:focus,.b a:hover{background-color:#3b5998;color:#fff;}.b .inv:focus,.b .inv:hover{background-color:#fff;color:#3b5998;}.ba{font-weight:normal;}.bd{height:20px;}.be{border-bottom:1px solid #ccd0d5;height:10px;text-align:center;width:100%;}.bf{color:#4b4f56;font-size:12px;line-height:20px;padding:0 10px;}.bg{background-color:#fff;}body{text-align:left;direction:ltr;}body,tr,input,textarea,button{font-family:sans-serif;}body,p,figure,h1,h2,h3,h4,h5,h6,ul,ol,li,dl,dd,dt{margin:0;padding:0;}h1,h2,h3,h4,h5,h6{font-size:1em;font-weight:bold;}ul,ol{list-style:none;}article,aside,figcaption,figure,footer,header,nav,section{display:block;}.d{background:#f5f6f7;}#page{position:relative;}.h{background-color:#3b5998;color:#fff;}.h ._4-i4{font-size:small;font-weight:normal;}.n{display:block;padding:5px;}._2xzg{background:#6d84b4;display:inline-block;font-size:14px;font-weight:normal;margin:4px 0;padding:2px 8px;}.b .i{border:0;border-collapse:collapse;margin:0;padding:0;width:100%;}.b .i tbody{vertical-align:top;}.b .j>tr>td,.b .j>tbody>tr>td,.b .i td.j{vertical-align:middle;}.b .i td{padding:0;}.b .q{width:100%;}.o,.o.p{display:block;}.l{display:block;}.m{height:20px;width:20px;}.p{border:0;display:inline-block;vertical-align:top;}i.p u{position:absolute;width:0;height:0;overflow:hidden;}body,tr,input,textarea,.e{font-size:medium;}/*]]>*/</style><meta content=\"Connectez-vous ou inscrivez-vous pour voir le contenu\" property=\"og:title\"/><meta content=\"Accédez aux publications, aux photos et plus encore sur Facebook.\" property=\"og:description\"/><link crossorigin=\"use-credentials\" href=\"/data/manifest/\" rel=\"manifest\"/></head><body class=\"b c d\" tabindex=\"0\"><div class=\"e\"><div id=\"viewport\"><div class=\"f g\" id=\"MChromeHeader\"><div class=\"h\" id=\"header\"><table align=\"center\" class=\"i j\"><tbody><tr><td class=\"k\"><a class=\"l m n\" href=\"/home.php?ref_component=mbasic_home_logo&amp;ref_page=%2Fflib%2Finit%2Fzeusgodofthunder%2F__entrypoint.php\"><img alt=\"Facebook logo\" class=\"o p\" height=\"20\" src=\"https://static.xx.fbcdn.net/rsrc.php/v3/ym/r/Gjhrhb7r0lb.png\" width=\"20\"/></a></td><td class=\"q\"><span>…<h1 class=\"_4-i4\"><a class=\"_2xzg inv\" href=\"/home.php\">Accéder à l’Accueil</a></h1></span></td></tr></tbody></table></div></div><div id=\"objects_container\"><div class=\"d\" id=\"root\"><div class=\"r\"><div class=\"s\">Rejoignez Facebook ou connectez-vous pour continuer.</div><div class=\"t\"><a class=\"u v w x y z ba\" href=\"/r.php?next=https%3A%2F%2Fmobile.facebook.com%2Fhttps%3A%2F%2Fmobile.facebook.com%2FZemmourEric%2F%2F&amp;cid=104\"><span class=\"bb\">Rejoindre</span></a><div class=\"bc\"><div class=\"bd\"><div class=\"be\"><span class=\"bf bg\">ou</span></div></div></div><a class=\"u v bh bi bj bk ba\" href=\"https://mobile.facebook.com/login.php?next=https%3A%2F%2Fmobile.facebook.com%2Fhttps%3A%2F%2Fmobile.facebook.com%2FZemmourEric%2F%2F&amp;ref=104&amp;refsrc=deprecated\"><span class=\"bb\">Se connecter</span></a></div></div><div style=\"display:none\"></div><span><img height=\"0\" src=\"https://facebook.com/security/hsts-pixel.gif?c=3.2\" style=\"display:none\" width=\"0\"/></span></div></div></div></div></body></html>\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'div'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-979db7b82162>\u001b[0m in \u001b[0;36mcrawl_profile\u001b[1;34m(self, profile_name, post_limit)\u001b[0m\n\u001b[0;32m    167\u001b[0m                 \u001b[0mposts_id\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'recent'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 168\u001b[1;33m                 \u001b[0mposts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprofile_bs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'div'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mid\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mposts_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdiv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdiv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontents\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    169\u001b[0m             \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'div'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-3a8473ade71f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mfscrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcrawl_profile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPROFILES_URL\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-27-979db7b82162>\u001b[0m in \u001b[0;36mcrawl_profile\u001b[1;34m(self, profile_name, post_limit)\u001b[0m\n\u001b[0;32m    169\u001b[0m             \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    170\u001b[0m                 \u001b[0mposts_id\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'structured_composer_async_container'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 171\u001b[1;33m                 \u001b[0mposts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprofile_bs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'div'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mid\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mposts_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdiv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdiv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontents\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    172\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    173\u001b[0m             \u001b[1;31m#get the posts url\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'div'"
     ]
    }
   ],
   "source": [
    "fscrapper.crawl_profile(PROFILES_URL[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:[*] Logged in.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'div'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-9339e72180ff>\u001b[0m in \u001b[0;36mcrawl_profile\u001b[1;34m(session, base_url, profile_url, post_limit)\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[0mposts_id\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'recent'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m             \u001b[0mposts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprofile_bs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'div'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mid\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mposts_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdiv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdiv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontents\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'div'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-9339e72180ff>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[0mposts_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mprofile_url\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mprofiles_urls\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 214\u001b[1;33m     \u001b[0mposts_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcrawl_profile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbase_url\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprofile_url\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m25\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    215\u001b[0m \u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'[!] Scraping finished. Total: {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mposts_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    216\u001b[0m \u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'[!] Saving.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-33-9339e72180ff>\u001b[0m in \u001b[0;36mcrawl_profile\u001b[1;34m(session, base_url, profile_url, post_limit)\u001b[0m\n\u001b[0;32m     53\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m             \u001b[0mposts_id\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'structured_composer_async_container'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m             \u001b[0mposts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprofile_bs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'div'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mid\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mposts_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdiv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdiv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontents\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m         \u001b[0mposts_urls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'href'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mprofile_bs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'a'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Full Story'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'div'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "import pandas\n",
    "from collections import OrderedDict\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "\n",
    "def get_bs(session, url):\n",
    "    \"\"\"Makes a GET requests using the given Session object\n",
    "    and returns a BeautifulSoup object.\n",
    "    \"\"\"\n",
    "    r = None\n",
    "    while True:\n",
    "        r = session.get(url)\n",
    "        if r.ok:\n",
    "            break\n",
    "    return BeautifulSoup(r.text, 'lxml')\n",
    "\n",
    "\n",
    "def make_login(session, base_url, credentials):\n",
    "    \"\"\"Returns a Session object logged in with credentials.\n",
    "    \"\"\"\n",
    "    login_form_url = '/login/device-based/regular/login/?refsrc=https%3A'\\\n",
    "        '%2F%2Fmobile.facebook.com%2Flogin%2Fdevice-based%2Fedit-user%2F&lwv=100'\n",
    "\n",
    "    params = {'email':credentials['email'], 'pass':credentials['pass']}\n",
    "\n",
    "    while True:\n",
    "        time.sleep(3)\n",
    "        logged_request = session.post(base_url+login_form_url, data=params)\n",
    "        \n",
    "        if logged_request.ok:\n",
    "            logging.info('[*] Logged in.')\n",
    "            break\n",
    "\n",
    "\n",
    "def crawl_profile(session, base_url, profile_url, post_limit):\n",
    "    \"\"\"Goes to profile URL, crawls it and extracts posts URLs.\n",
    "    \"\"\"\n",
    "    profile_bs = get_bs(session, profile_url)\n",
    "    n_scraped_posts = 0\n",
    "    scraped_posts = list()\n",
    "    posts_id = None\n",
    "\n",
    "    while n_scraped_posts < post_limit:\n",
    "        try:\n",
    "            posts_id = 'recent'\n",
    "            posts = profile_bs.find('div', id=posts_id).div.div.contents\n",
    "        except Exception:\n",
    "            posts_id = 'structured_composer_async_container'\n",
    "            posts = profile_bs.find('div', id=posts_id).div.div.contents\n",
    "\n",
    "        posts_urls = [a['href'] for a in profile_bs.find_all('a', text='Full Story')] \n",
    "\n",
    "        for post_url in posts_urls:\n",
    "            # print(post_url)\n",
    "            try:\n",
    "                post_data = scrape_post(session, base_url, post_url)\n",
    "                scraped_posts.append(post_data)\n",
    "            except Exception as e:\n",
    "                logging.info('Error: {}'.format(e))\n",
    "            n_scraped_posts += 1\n",
    "            if posts_completed(scraped_posts, post_limit):\n",
    "                break\n",
    "        \n",
    "        show_more_posts_url = None\n",
    "        if not posts_completed(scraped_posts, post_limit):\n",
    "            show_more_posts_url = profile_bs.find('div', id=posts_id).next_sibling.a['href']\n",
    "            profile_bs = get_bs(session, base_url+show_more_posts_url)\n",
    "            time.sleep(3)\n",
    "        else:\n",
    "            break\n",
    "    return scraped_posts\n",
    "\n",
    "def posts_completed(scraped_posts, limit):\n",
    "    \"\"\"Returns true if the amount of posts scraped from\n",
    "    profile has reached its limit.\n",
    "    \"\"\"\n",
    "    if len(scraped_posts) == limit:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "def scrape_post(session, base_url, post_url):\n",
    "    \"\"\"Goes to post URL and extracts post data.\n",
    "    \"\"\"\n",
    "    post_data = OrderedDict()\n",
    "\n",
    "    post_bs = get_bs(session, base_url+post_url)\n",
    "    time.sleep(5)\n",
    "\n",
    "    # Here we populate the OrderedDict object\n",
    "    post_data['url'] = post_url\n",
    "\n",
    "    try:\n",
    "        post_text_element = post_bs.find('div', id='u_0_0').div\n",
    "        string_groups = [p.strings for p in post_text_element.find_all('p')]\n",
    "        strings = [repr(string) for group in string_groups for string in group]\n",
    "        post_data['text'] = strings\n",
    "    except Exception:\n",
    "        post_data['text'] = []\n",
    "    \n",
    "    try:\n",
    "        post_data['media_url'] = post_bs.find('div', id='u_0_0').find('a')['href']\n",
    "    except Exception:\n",
    "        post_data['media_url'] = ''\n",
    "    \n",
    "\n",
    "    try:\n",
    "        post_data['comments'] = extract_comments(session, base_url, post_bs, post_url)\n",
    "    except Exception:\n",
    "        post_data['comments'] = []\n",
    "    \n",
    "    return dict(post_data)\n",
    "\n",
    "\n",
    "def extract_comments(session, base_url, post_bs, post_url):\n",
    "    \"\"\"Extracts all coments from post\n",
    "    \"\"\"\n",
    "    comments = list()\n",
    "    show_more_url = post_bs.find('a', href=re.compile('/story\\.php\\?story'))['href']\n",
    "    first_comment_page = True\n",
    "\n",
    "    logging.info('Scraping comments from {}'.format(post_url))\n",
    "    while True:\n",
    "\n",
    "        logging.info('[!] Scraping comments.')\n",
    "        time.sleep(3)\n",
    "        if first_comment_page:\n",
    "            first_comment_page = False\n",
    "        else:\n",
    "            post_bs = get_bs(session, base_url+show_more_url)\n",
    "            time.sleep(3)\n",
    "        \n",
    "        try:\n",
    "            comments_elements = post_bs.find('div', id=re.compile('composer')).next_sibling\\\n",
    "                .find_all('div', id=re.compile('^\\d+'))\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        if len(comments_elements) != 0:\n",
    "            logging.info('[!] There are comments.')\n",
    "        else:\n",
    "            break\n",
    "        \n",
    "        for comment in comments_elements:\n",
    "            comment_data = OrderedDict()\n",
    "            comment_data['text'] = list()\n",
    "            try:\n",
    "                comment_strings = comment.find('h3').next_sibling.strings\n",
    "                for string in comment_strings:\n",
    "                    comment_data['text'].append(string)\n",
    "            except Exception:\n",
    "                pass\n",
    "            \n",
    "            try:\n",
    "                media = comment.find('h3').next_sibling.next_sibling.children\n",
    "                if media is not None:\n",
    "                    for element in media:\n",
    "                        comment_data['media_url'] = element['src']\n",
    "                else:\n",
    "                    comment_data['media_url'] = ''\n",
    "            except Exception:\n",
    "                pass\n",
    "            \n",
    "            comment_data['profile_name'] = comment.find('h3').a.string\n",
    "            comment_data['profile_url'] = comment.find('h3').a['href'].split('?')[0]\n",
    "            comments.append(dict(comment_data))\n",
    "        \n",
    "        show_more_url = post_bs.find('a', href=re.compile('/story\\.php\\?story'))\n",
    "        if 'View more' in show_more_url.text:\n",
    "            logging.info('[!] More comments.')\n",
    "            show_more_url = show_more_url['href']\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    return comments\n",
    "\n",
    "\n",
    "def json_to_obj(filename):\n",
    "    \"\"\"Extracts dta from JSON file and saves it on Python object\n",
    "    \"\"\"\n",
    "    obj = None\n",
    "    with open(filename) as json_file:\n",
    "        obj = json.loads(json_file.read())\n",
    "    return obj\n",
    "\n",
    "\n",
    "def save_data(data):\n",
    "    \"\"\"Converts data to JSON.\n",
    "    \"\"\"\n",
    "    with open('profile_posts_data.json', 'w') as json_file:\n",
    "        json.dump(data, json_file, indent=4)\n",
    "\n",
    "\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "base_url = 'https://mobile.facebook.com'\n",
    "session = requests.session()\n",
    "\n",
    "# Extracts credentials for the login and all of the profiles URL to scrape\n",
    "credentials = CREDENTIALS\n",
    "profiles_urls = PROFILES_URL\n",
    "\n",
    "make_login(session, base_url, credentials)\n",
    "\n",
    "posts_data = None\n",
    "for profile_url in profiles_urls:\n",
    "    posts_data = crawl_profile(session, base_url, profile_url, 25)\n",
    "logging.info('[!] Scraping finished. Total: {}'.format(len(posts_data)))\n",
    "logging.info('[!] Saving.')\n",
    "save_data(posts_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
